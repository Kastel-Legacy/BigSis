import requests
from typing import List, Dict
from core.config import settings
from core.rag.ingestion import ingest_document
import asyncio

BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"

def search_pubmed(query: str, max_results: int = None) -> List[str]:
    print(f"   ... Appel API PubMed Search pour: {query}")
    limit = max_results if max_results else (settings.MAX_STUDIES_PER_RUN + 2)
    params = {
        "db": "pubmed",
        "term": query,
        "retmode": "json",
        "retmax": limit, 
        "reldate": settings.SEARCH_DAYS_BACK,
        "email": settings.PUBMED_EMAIL
    }

    try:
        resp = requests.get(f"{BASE_URL}/esearch.fcgi", params=params)
        resp.raise_for_status()
        data = resp.json()
        return data.get("esearchresult", {}).get("idlist", [])
    except Exception as e:
        print(f"âš ï¸ Erreur PubMed Search: {e}")
        return []

import xml.etree.ElementTree as ET

def fetch_details(pmids: List[str]) -> List[Dict]:
    if not pmids:
        return []
    
    ids_str = ",".join(pmids)
    print(f"   ... RÃ©cupÃ©ration dÃ©tails complets (efetch) pour {len(pmids)} ID(s)")
    
    # Use efetch for full records (including abstracts)
    params = {
        "db": "pubmed",
        "id": ids_str,
        "retmode": "xml", 
        "email": settings.PUBMED_EMAIL
    }
    
    try:
        resp = requests.get(f"{BASE_URL}/efetch.fcgi", params=params)
        resp.raise_for_status()
        
        root = ET.fromstring(resp.content)
        docs = []
        
        for article in root.findall(".//PubmedArticle"):
            try:
                medline = article.find("MedlineCitation")
                pmid = medline.find("PMID").text
                
                article_data = medline.find("Article")
                title = article_data.find("ArticleTitle").text
                
                # Extract Abstract
                abstract_elem = article_data.find("Abstract")
                abstract_text = ""
                if abstract_elem is not None:
                    # Abstracts can have multiple parts (Background, Methods, etc.)
                    parts = []
                    for text_node in abstract_elem.findall("AbstractText"):
                        label = text_node.get("Label")
                        text = text_node.text
                        if text:
                            if label:
                                parts.append(f"{label}: {text}")
                            else:
                                parts.append(text)
                    abstract_text = "\n".join(parts)
                
                if not abstract_text:
                    abstract_text = "Abstract non disponible."

                # Extract Year
                journal = article_data.find("Journal")
                journal_issue = journal.find("JournalIssue")
                pub_date = journal_issue.find("PubDate")
                year_elem = pub_date.find("Year")
                if year_elem is None:
                    # Fallback to MedlineDate if Year is missing
                    medline_date = pub_date.find("MedlineDate")
                    year = medline_date.text[:4] if medline_date is not None else "N/A"
                else:
                    year = year_elem.text
                    
                docs.append({
                    "pmid": pmid,
                    "titre": title,
                    "resume": abstract_text,
                    "annee": year,
                    "lien": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                })
            except Exception as parse_e:
                print(f"âš ï¸ Erreur parsing article: {parse_e}")
                continue
                
        return docs
    except Exception as e:
        print(f"âš ï¸ Erreur PubMed Details: {e}")
        return []

async def ingest_pubmed_results(query: str):
    """
    Search PubMed for the query, fetch details, and ingest into RAG.
    """
    print(f"ğŸš€ DÃ©marrage recherche PubMed: {query}")
    
    # 1. Search
    pmids = search_pubmed(query)
    if not pmids:
        print("Aucun article trouvÃ©.")
        return 0
        
    print(f"TrouvÃ© {len(pmids)} articles. RÃ©cupÃ©ration des dÃ©tails...")
    
    # 2. Fetch Details
    # We might want to batch this if there are many IDs, but start simple
    docs = fetch_details(pmids)
    
    # 3. Ingest
    count = 0
    for doc in docs:
        content = f"Titre: {doc['titre']}\n\nAbstract:\n{doc['resume']}\n\nJournal/AnnÃ©e: {doc['annee']}\nLien: {doc['lien']}"
        
        metadata = {
            "source": "pubmed",
            "pmid": doc['pmid'],
            "year": doc['annee'],
            "url": doc['lien'] # Use 'url' key for ingestion.py
        }
        
        # Ingest
        await ingest_document(
            title=doc['titre'],
            content=content,
            metadata=metadata
        )
        count += 1
        
    print(f"âœ… Ingestion terminÃ©e pour {count} articles.")
    return count

async def search_claims_for_ingredient(ingredient: str) -> List[Dict]:
    """
    Specific search for 'efficacy' and 'safety' claims.
    Returns raw docs (title, abstract, pmid).
    """
    print(f"ğŸ” Recherche de PREUVES pour: {ingredient}")
    
    # 1. Construct Queries
    # Efficacy: (Ingredient) AND (Skin OR Dermatology) AND (Efficacy OR Effectiveness OR Treatment)
    # Safety: (Ingredient) AND (Skin OR Dermatology) AND (Safety OR Side Effects OR Adverse)
    
    base_term = f'("{ingredient}"[Title/Abstract]) AND ("skin"[MeSH Terms] OR "dermatology"[MeSH Terms])'
    
    queries = [
        f'{base_term} AND ("efficacy"[Title/Abstract] OR "effectiveness"[Title/Abstract]) AND (Meta-Analysis[pt] OR Systematic Review[pt] OR Randomized Controlled Trial[pt])',
        f'{base_term} AND ("safety"[Title/Abstract] OR "adverse effects"[Title/Abstract] OR "toxicity"[Title/Abstract])'
    ]
    
    all_pmids = set()
    
    # 2. Search
    for q in queries:
        pmids = search_pubmed(q)
        all_pmids.update(pmids)
        
    if not all_pmids:
        print(f"   -> Aucune preuve trouvÃ©e pour {ingredient}")
        return []

    # 3. Fetch Details
    # Limit to top 5 most relevant/recent combined
    top_pmids = list(all_pmids)[:5] 
    docs = fetch_details(top_pmids)
    
    print(f"   -> TrouvÃ© {len(docs)} documents pertinents.")
    return docs
